{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plt_colors\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "import scipy.stats as st\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "from holoviews import dim\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc('xtick', labelsize=14)     \n",
    "matplotlib.rc('ytick', labelsize=14)\n",
    "matplotlib.rc('axes', labelsize=14, titlesize=14)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from counts_analysis.c_utils import COUNTS_CSV, CLASSES, set_settings, set_counts\n",
    "\n",
    "#== Load Datasets ==#\n",
    "df = pd.read_csv(COUNTS_CSV['counts'])\n",
    "# Dataset without problematic classes (Gyrodinium, Pseudo-nitzchia chain)\n",
    "df_ = df[df['class'].isin(CLASSES)].reset_index(drop=True)\n",
    "data = df.copy()\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#=== Set count forms & settings ===#\n",
    "# COUNT\n",
    "# SETTING\n",
    "# Original raw counts\n",
    "rc_counts = set_counts('gtruth', 'raw count', micro_default=True)\n",
    "rc_counts_pred = set_counts('predicted', 'raw count', micro_default=True)\n",
    "rc_settings = set_settings(rc_counts)\n",
    "print('Example of setting\\n{}'.format(rc_settings))\n",
    "# Relative abundance\n",
    "rel_counts = set_counts('gtruth', 'relative abundance', micro_default=False)\n",
    "rel_counts = ['micro cells/mL relative abundance'] + list(rel_counts[1:])\n",
    "# Classifier predicted counts\n",
    "rel_counts_pred = set_counts('predicted', 'relative abundance', micro_default=False)\n",
    "rel_counts_pred = ['micro cells/mL relative abundance'] + list(rel_counts_pred[1:])\n",
    "\n",
    "#=== Set classifier gtruth vs predictions\n",
    "lab_gtruth_pred = ['lab {} raw count'.format(lbl) for lbl in ['gtruth', 'predicted']]\n",
    "pier_gtruth_pred = ['pier {} raw count'.format(lbl) for lbl in ['gtruth', 'predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))\n",
    "    \n",
    "def compute_relative_abundance(raw_count, data):\n",
    "    if 'micro' in raw_count:\n",
    "        relative_column = 'micro cells/mL relative abundance'\n",
    "    else:\n",
    "        relative_column = f'{raw_count.split()[0]} {raw_count.split()[1]} relative abundance'\n",
    "    data[relative_column] = data.groupby('class')[raw_count].apply(lambda x: x / x.sum() * 100.0 if sum(x) != 0 else x)\n",
    "    return data\n",
    "\n",
    "def filter_classes(df, classes):\n",
    "    return df[~df['class'].isin(classes)].reset_index(drop=True)\n",
    "\n",
    "def load_baseline_dataset(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Filter classes\n",
    "    df = filter_classes(df, ['Gyrodinium', 'Ceratium falcatiforme or fusus', 'Chattonella', 'Pseudo-nitzschia chain'])\n",
    "    return df\n",
    "\n",
    "def load_rel_class_sum_dataset(data):\n",
    "    df = data.copy()\n",
    "    # Filter classes\n",
    "    df = filter_classes(df, ['Gyrodinium', 'Ceratium falcatiforme or fusus', 'Chattonella', 'Pseudo-nitzschia chain'])\n",
    "\n",
    "    # Compute relative abundance\n",
    "    for rc in list(rc_counts + rc_counts_pred):\n",
    "        df = compute_relative_abundance(rc, df)\n",
    "    return df\n",
    "\n",
    "def load_seasonal_dataset(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    # Filter classes\n",
    "    df = filter_classes(df, ['Gyrodinium', 'Pseudo-nitzschia chain'])\n",
    "\n",
    "    # Compute relative abundance\n",
    "    for rc in list(rc_counts + rc_counts_pred):\n",
    "        df = compute_relative_abundance(rc, df)\n",
    "\n",
    "    # Separate into seasonal/nonseasonal dates\n",
    "    dates = ['2019-05-23', '2019-05-28', '2019-06-03']\n",
    "    seasonal = df[df['datetime'].isin(dates)]\n",
    "    nonseasonal = df[~df['datetime'].isin(dates)]\n",
    "    return seasonal, nonseasonal\n",
    "\n",
    "baseline = load_baseline_dataset(df.copy())\n",
    "rel_class_sum = load_rel_class_sum_dataset(df.copy())\n",
    "seasonal, nonseasonal = load_seasonal_dataset(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab vs Pier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== plot distributions ===#\n",
    "y = seasonal.copy()\n",
    "dataset_type = 'seasonal'.upper()\n",
    "from counts_analysis.plot_class_summary import plot_summary_sampling_class_dist\n",
    "# printmd('Original Relative Abundance')\n",
    "# plot_summary_sampling_class_dist(df, rel_counts, False)\n",
    "printmd(f'### {dataset_type} Camera Distribution')\n",
    "plot_summary_sampling_class_dist(y, rel_counts, False, relative=True)\n",
    "\n",
    "printmd(f'### {dataset_type} Automated Classifier Counts Distribution')\n",
    "plot_summary_sampling_class_dist(y, rel_counts_pred, False, relative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = seasonal.copy()\n",
    "\n",
    "from validate_exp.stat_fns import mase, investigate_mase, pearson, concordance_correlation_coefficient\n",
    "\n",
    "# Set evaluation metric\n",
    "stat = mase\n",
    "\n",
    "# Set settings\n",
    "settings_ = [set_settings(count) for count in [rel_counts, rel_counts_pred]]\n",
    "count_forms = dict(zip(['relative', 'relative predicted'], settings_))\n",
    "\n",
    "from eval_counts import compare_count_forms\n",
    "\n",
    "# Evaluate count forms\n",
    "printmd(f'# {dataset_type} MASE')\n",
    "settings_score = compare_count_forms(count_forms, stat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Lab vs Pier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(settings_score[settings_score['count form'] == 'relative'])\n",
    "settings_score[settings_score['count form'] == 'relative'][['class', 'lab - micro','pier - micro','pier - lab']].hvplot.bar(x='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes that increase in error from `lab - micro`: \n",
    "\n",
    "Akashiwo, Lingulodinium Polyedra, Prorocentrum micans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from counts_analysis.plot_class_summary import plot_summary_both_count_forms, plot_class_summary\n",
    "\n",
    "def filter_class(cls, x_data, y_data):\n",
    "    x_cls_df = x_data[x_data['class'] == cls].reset_index(drop=True)\n",
    "    y_cls_df = y_data[y_data['class'] == cls].reset_index(drop=True)\n",
    "    return x_cls_df, y_cls_df\n",
    "                     \n",
    "def plot_summary(x, y):\n",
    "    x_count, x_data, x_relative = x\n",
    "    y_count, y_data, y_relative = y\n",
    "    datetime_col = ['datetime']\n",
    "    display_side_by_side([x_data[datetime_col + list(x_count)], y_data[datetime_col + list(y_count)]], ['raw', 'relative'])\n",
    "    printmd(f'### Sum total over N={x_data[\"datetime\"].nunique()} days')\n",
    "    display(x_data[list(x_count)].sum())\n",
    "    display_side_by_side([x_data[list(x_count)].describe(), y_data[list(y_count)].describe()], ['raw descriptors', 'relative descriptors'])\n",
    "    return hv.Layout(plot_class_summary(x_count, x_data, relative=x_relative) + plot_class_summary(y_count, y_data, relative=y_relative)).cols(3).opts(shared_axes=False)\n",
    "\n",
    "def compute_relative_abundance(raw_count, data):\n",
    "    if 'micro' in raw_count:\n",
    "        relative_column = 'micro cells/mL relative abundance'\n",
    "    else:\n",
    "        relative_column = f'{raw_count.split()[0]} {raw_count.split()[1]} relative abundance'\n",
    "    data[relative_column] = data.groupby('class')[raw_count].apply(lambda x: x / x.sum() * 100.0 if sum(x) != 0 else x)\n",
    "    return data\n",
    "    \n",
    "def _plot_class_summary(cls, x, y, x_relative=False, y_relative=True, classifier=False):\n",
    "    x_df, y_df = filter_class(cls, x, y)\n",
    "    printmd(f'# {cls}')\n",
    "    if classifier:\n",
    "        x_counts, y_counts = rc_counts_pred, rel_counts_pred\n",
    "    else:\n",
    "        x_counts, y_counts = rc_counts, rel_counts\n",
    "    return plot_summary((x_counts, x_df, x_relative), (y_counts, y_df, y_relative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = 'Akashiwo'\n",
    "_plot_class_summary(cls, seasonal, seasonal, x_relative=False, y_relative=True, classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mase(cls_df, x, y, error_name):\n",
    "    ms = investigate_mase(cls_df, gtruth=x, pred=y)\n",
    "    ms[error_name] = ms['error'] / ms['naive']\n",
    "    return ms\n",
    "\n",
    "LM_MASE = 'MASE (lab - micro)'\n",
    "PM_MASE = 'MASE (pier - micro)'\n",
    "PL_MASE = 'MASE (pier - lab)'\n",
    "\n",
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=rel_counts[1], y=rel_counts[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])}\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (lab - micro): {np.mean(pl[PL_MASE])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = 'Lingulodinium polyedra'\n",
    "_plot_class_summary(cls, seasonal, seasonal, x_relative=False, y_relative=True, classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=rel_counts[1], y=rel_counts[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])}\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (lab - micro): {np.mean(pl[PL_MASE])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = 'Prorocentrum micans'\n",
    "_plot_class_summary(cls, seasonal, seasonal, x_relative=False, y_relative=True, classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=rel_counts[1], y=rel_counts[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])}\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (lab - micro): {np.mean(pl[PL_MASE])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))\n",
    "    \n",
    "def _compute_mase(cls_df, x, y, error_name):\n",
    "    ms = investigate_mase(cls_df, gtruth=x, pred=y)\n",
    "    ms[error_name] = ms['error'] / ms['naive']\n",
    "    return ms\n",
    "\n",
    "LM_MASE = 'MASE (lab - micro)'\n",
    "PM_MASE = 'MASE (pier - micro)'\n",
    "PL_MASE = 'MASE (pier - lab)'\n",
    "\n",
    "scaled_error = pd.DataFrame()\n",
    "for cls, cls_df in y.groupby('class'):\n",
    "#     if cls in ['Ceratium falcatiforme or fusus', 'Ceratium furca', 'Cochlodinium']:\n",
    "#         continue\n",
    "        \n",
    "    lm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[1], error_name=LM_MASE)\n",
    "    pm = _compute_mase(cls_df, x=rel_counts[0], y=rel_counts[2], error_name=PM_MASE)\n",
    "    pl = _compute_mase(cls_df, x=rel_counts[1], y=rel_counts[2], error_name=PL_MASE)\n",
    "    printmd(f'# {cls}')\n",
    "    printmd('### error lab - micro')\n",
    "    display(lm)\n",
    "    print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])}\\n')\n",
    "    printmd('### error pier - lab')\n",
    "    display(pl)\n",
    "    print(f'Final Score (lab - micro): {np.mean(pl[PL_MASE])}\\n')\n",
    "\n",
    "    cls_error = lm.merge(pm, on=['class', 'datetime'])\n",
    "    cls_error = cls_error.merge(pl, on=['class', 'datetime'])\n",
    "\n",
    "    scaled_error = scaled_error.append(cls_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of stats above\n",
    "It seems most of the error inflation is due to the use of the lab as the new objective ground truth. We see this the most with the Akashiwo. \n",
    "\n",
    "The relative abundances appear to be quite low. This suggests that there's a discrepancy between how much the pier collects vs the lab & micro. \n",
    "\n",
    "We also know that this is a relatively rare species that occured during a seasonal event of Lingulodinium polyedra and Prorocentrum micans, meaning it's almost expected that it'd be difficult for the microscopy to detect it. \n",
    "\n",
    "Thus it could be a combination of switching the objective ground truth to a system that is less able to precisely estimate similar counts. The pier is a level better than it but less than the micro.\n",
    "\n",
    "- Microscopy better reflected with the pier than the lab because more precise/correlated counts for these dominant species. micro and lab agree moreso because of the amount of rare species. lab and pier is the worse of the two, because the lab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_error.hvplot.heatmap(x='datetime', y='class', C=LM_MASE, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_error.hvplot.heatmap(x='datetime', y='class', C=PL_MASE, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Seasonal (Class Relative Abundance)\n",
    "\"\"\"\n",
    "y = seasonal.copy()\n",
    "dataset_type = 'seasonal'.upper()\n",
    "printmd(f'# {dataset_type} ERROR ANALYSIS')\n",
    "printmd('Camera Counts')\n",
    "display(y[['class', 'datetime'] + list(rel_counts)])\n",
    "# printmd('Automated Classifier Counts')\n",
    "# display(y[['class', 'datetime'] + list(rel_counts_pred)].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Investigate Class Errors over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Scatter [tools=['hover'], legend_position='left', color_index='class', width=700, height=500, logx=True]\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def plot_class_mase_scores_vs_gtruth_count_for_each_setting(sampled_data,\n",
    "                                                            score_settings,\n",
    "                                                            interactive=False):\n",
    "    if not interactive:\n",
    "        current_palette_7 = sns.color_palette(\"coolwarm\", 9)\n",
    "        sns.set_palette(current_palette_7)\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        i = 0\n",
    "        for label, (gtruth_setting, experimental_setting) in score_settings.items():\n",
    "            s = sns.scatterplot(x=gtruth_setting, y=label, hue='class',\n",
    "                                data=sampled_data, ax=ax[i], s=50)\n",
    "            ax[i].set_xscale('symlog')\n",
    "            ax[i].set_ylabel('MASE')\n",
    "            ax[i].set_xlabel(\n",
    "                \"Logged Gtruth Counts ({})\".format(get_units(gtruth_setting)))\n",
    "            ax[i].set_title(label)\n",
    "            if i <= 1:\n",
    "                s.legend_.remove()\n",
    "            i += 1\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "    else:\n",
    "        label = LM_MASE\n",
    "        sc1 = hv.Scatter(sampled_data, score_settings[label][0],\n",
    "                         [label, 'class', 'datetime'])\n",
    "        sc1 = sc1.redim.range(gtruth_setting=(0, None))\n",
    "        sc1 = sc1.opts(cmap='coolwarm', size=7, title=label)\n",
    "\n",
    "        label = PM_MASE\n",
    "        sc2 = hv.Scatter(sampled_data, score_settings[label][0],\n",
    "                         [label, 'class', 'datetime'])\n",
    "        sc2 = sc2.redim.range(gtruth_setting=(0, None))\n",
    "        sc2 = sc2.opts(cmap='coolwarm', size=7, title=label)\n",
    "\n",
    "        label = PL_MASE\n",
    "        sc3 = hv.Scatter(sampled_data, score_settings[label][0],\n",
    "                         [label, 'class', 'datetime'])\n",
    "        sc3 = sc3.redim.range(gtruth_setting=(0, None))\n",
    "        sc3 = sc3.opts(cmap='coolwarm', size=7, title=label)\n",
    "\n",
    "        return sc1, sc2, sc3\n",
    "\n",
    "scaled_error = scaled_error.rename({'micro cells/mL relative abundance_x': 'micro cells/mL relative abundance',\n",
    "                               'lab gtruth relative abundance_x': 'lab gtruth relative abundance' }, axis=1)\n",
    "\n",
    "data = scaled_error.copy()\n",
    "scores_df = defaultdict(list)\n",
    "counts = rel_counts\n",
    "\n",
    "score_settings = {LM_MASE: (counts[0], counts[1]),\n",
    "                  PM_MASE: (counts[0], counts[2]),\n",
    "                  PL_MASE: (counts[1], counts[2])}\n",
    "\n",
    "\n",
    "# plot_class_mase_scores_vs_gtruth_count_for_each_setting(sampled_data, score_settings)\n",
    "\n",
    "print('\\nInteractive')\n",
    "sc1, sc2, sc3 = plot_class_mase_scores_vs_gtruth_count_for_each_setting(data,\n",
    "                                                                        score_settings,\n",
    "                                                                        interactive=True)\n",
    "hv.Layout(sc1 + sc2 + sc3).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts HeatMap[colorbar=True, width=1000, height=300, xrotation=60, tools=['hover'], shared_axes=True]\n",
    "# PLOT SMAPE Class over Time\n",
    "\n",
    "data = data.sort_values(['datetime', 'class'])\n",
    "sdata = hv.Dataset(data=data, kdims=['class', 'datetime'])\n",
    "\n",
    "label = LM_MASE\n",
    "t1 = sdata.to(hv.HeatMap, ['datetime', 'class'], label).opts(title=label)\n",
    "\n",
    "label = PM_MASE\n",
    "t2 = sdata.to(hv.HeatMap, ['datetime', 'class'], label).opts(title=label)\n",
    "\n",
    "label = PL_MASE\n",
    "t3 = sdata.to(hv.HeatMap, ['datetime', 'class'], label).opts(title=label)\n",
    "\n",
    "hv.Layout(t1 + t2 + t3).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data above^, it seems that our error for the `pier - lab` is due to the rare classes causing the error to increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate count forms\n",
    "printmd(f'# {dataset_type} Pearson')\n",
    "settings_score = compare_count_forms(count_forms, pearson, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonSeasonal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# NonSeasonal (Class Relative Abundance)\n",
    "\"\"\"\n",
    "y = nonseasonal.copy()\n",
    "dataset_type = 'nonseasonal'.upper()\n",
    "printmd(f'# {dataset_type} ERROR ANALYSIS')\n",
    "printmd('Camera Counts')\n",
    "display(y[['class', 'datetime'] + list(rel_counts)].head(10))\n",
    "printmd('Automated Classifier Counts')\n",
    "display(y[['class', 'datetime'] + list(rel_counts_pred)].head(10))\n",
    "\n",
    "# #=== plot distributions ===#\n",
    "# from counts_analysis.plot_class_summary import plot_summary_sampling_class_dist\n",
    "# # printmd('Original Relative Abundance')\n",
    "# # plot_summary_sampling_class_dist(df, rel_counts, False)\n",
    "# printmd(f'### {dataset_type} Camera Distribution')\n",
    "# plot_summary_sampling_class_dist(y, rel_counts, False, relative=True)\n",
    "\n",
    "# printmd(f'### {dataset_type} Automated Classifier Counts Distribution')\n",
    "# plot_summary_sampling_class_dist(y, rel_counts_pred, False, relative=True)\n",
    "\n",
    "from validate_exp.stat_fns import mase, investigate_mase, pearson, concordance_correlation_coefficient\n",
    "\n",
    "ms = investigate_mase(y.groupby('class').get_group('Prorocentrum micans'), gtruth=rel_counts[0], pred=rel_counts[1])\n",
    "ms['scaled_error'] = ms['error'] / ms['naive']\n",
    "# display(ms)\n",
    "# print(np.mean(ms['scaled_error']))\n",
    "\n",
    "# Set evaluation metric\n",
    "stat = mase\n",
    "\n",
    "# Set settings\n",
    "settings_ = [set_settings(count) for count in [rel_counts, rel_counts_pred]]\n",
    "count_forms = dict(zip(['relative', 'relative predicted'], settings_))\n",
    "\n",
    "from eval_counts import compare_count_forms\n",
    "\n",
    "# Evaluate count forms\n",
    "printmd(f'# {dataset_type} MASE')\n",
    "settings_score = compare_count_forms(count_forms, stat, y)\n",
    "\n",
    "# Evaluate count forms\n",
    "printmd(f'# {dataset_type} Pearson')\n",
    "settings_score = compare_count_forms(count_forms, pearson, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))\n",
    "    \n",
    "def investigate_mase(df, x, y):\n",
    "    gtruth = rel_counts[x]\n",
    "    pred = rel_counts[y]\n",
    "    temp = df[['class', 'datetime', rc_counts[x], gtruth, rc_counts[y], pred]]\n",
    "    temp['error'] = np.abs(temp[gtruth] - temp[pred])\n",
    "    temp['naive'] = np.mean(np.abs(np.diff(temp[gtruth])))\n",
    "    return temp\n",
    "    \n",
    "def _compute_mase(cls_df, x, y, error_name):\n",
    "    ms = investigate_mase(cls_df, x=x, y=y)\n",
    "    ms[error_name] = ms['error'] / ms['naive']\n",
    "    return ms\n",
    "\n",
    "LM_MASE = 'MASE (lab - micro)'\n",
    "PM_MASE = 'MASE (pier - micro)'\n",
    "PL_MASE = 'MASE (pier - lab)'\n",
    "\n",
    "scaled_error1 = pd.DataFrame()\n",
    "for cls, cls_df in y.groupby('class'):\n",
    "        \n",
    "    lm = _compute_mase(cls_df, x=0, y=1, error_name=LM_MASE)\n",
    "    pm = _compute_mase(cls_df, x=0, y=2, error_name=PM_MASE)\n",
    "    pl = _compute_mase(cls_df, x=1, y=2, error_name=PL_MASE)\n",
    "    \n",
    "    if cls in ['Akashiwo']:\n",
    "        printmd(f'# {cls}')\n",
    "        printmd('### error lab - micro')\n",
    "        display(lm.sort_values(by='error'))\n",
    "        print(f'Sum counts (lab - micro):\\n{cls_df[[rc_counts[0], rc_counts[1]]].sum()}')\n",
    "        print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])}\\n')\n",
    "        printmd('### error pier - micro')\n",
    "        display(pm.sort_values(by='error'))\n",
    "        print(f'Sum counts (pier - micro):\\n{cls_df[[rc_counts[0], rc_counts[2]]].sum()}')\n",
    "        print(f'Final Score (pier - micro): {np.mean(pm[PM_MASE])}\\n')\n",
    "        printmd('### error pier - lab')\n",
    "        display(pl.sort_values(by='error'))\n",
    "        print(f'Sum counts (pier - lab):\\n{cls_df[[rc_counts[1], rc_counts[2]]].sum()}')\n",
    "        print(f'Final Score (pier - lab): {np.mean(pl[PL_MASE])}\\n')\n",
    "\n",
    "    cls_error = lm.merge(pm, on=['class', 'datetime'])\n",
    "    cls_error = cls_error.merge(pl, on=['class', 'datetime'])\n",
    "\n",
    "    scaled_error1 = scaled_error1.append(cls_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts HeatMap[colorbar=True, width=1000, height=300, xrotation=60, tools=['hover'], shared_axes=True]\n",
    "\n",
    "import hvplot.pandas\n",
    "\n",
    "scaled_error1 = scaled_error.copy()\n",
    "\n",
    "for ms in [LM_MASE, PM_MASE, PL_MASE]:\n",
    "    condition = scaled_error1[ms] > 1.0\n",
    "    scaled_error1.loc[condition, ms] = 1.0\n",
    "\n",
    "h1 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=LM_MASE, cmap='coolwarm').opts(title=LM_MASE)\n",
    "h2 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=PM_MASE, cmap='coolwarm').opts(title=PM_MASE)\n",
    "h3 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=PL_MASE, cmap='coolwarm').opts(title=PL_MASE)\n",
    "hv.Layout(h1 + h2 + h3).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = y[['datetime', 'class'] + list(rel_counts)]\n",
    "counts_df = counts_df.melt(id_vars=['class', 'datetime'], var_name=['setting'], value_name='relative abundance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts HeatMap[colorbar=True, width=1000, height=300, xrotation=60, tools=['hover'], shared_axes=True]\n",
    "\n",
    "import hvplot.pandas\n",
    "\n",
    "counts_df = y.copy()\n",
    "counts_df = counts_df.sort_values(by=['class', 'datetime'])\n",
    "sdata = hv.Dataset(data=counts_df, kdims=['class', 'datetime'])\n",
    "\n",
    "label = rel_counts[0]\n",
    "t = sdata.to(hv.HeatMap, ['datetime', 'class'], label).opts(title=label)\n",
    "\n",
    "label = rel_counts[1]\n",
    "t1 = sdata.to(hv.HeatMap, ['datetime', 'class'], label).opts(title=label)\n",
    "\n",
    "label = rel_counts[2]\n",
    "t2 = sdata.to(hv.HeatMap, ['datetime', 'class'], label).opts(title=label)\n",
    "\n",
    "hv.Layout(t+t1+t2).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Scatter [tools=['hover'], legend_position='left', color_index='class', width=700, height=500, logx=True]\n",
    "\n",
    "def plot_class_mase_scores_vs_gtruth_count_for_each_setting(sampled_data,\n",
    "                                                            score_settings,\n",
    "                                                            interactive=False):\n",
    "    if not interactive:\n",
    "        current_palette_7 = sns.color_palette(\"coolwarm\", 7)\n",
    "        sns.set_palette(current_palette_7)\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        i = 0\n",
    "        for label, (gtruth_setting, experimental_setting) in score_settings.items():\n",
    "            s = sns.scatterplot(x=gtruth_setting, y=label, hue='class',\n",
    "                                data=sampled_data, ax=ax[i], s=50)\n",
    "            ax[i].set_yscale('symlog')\n",
    "            ax[i].set_xscale('symlog')\n",
    "            ax[i].set_ylabel('MASE')\n",
    "            ax[i].set_xlabel(\n",
    "                \"Logged Gtruth Counts ({})\".format(get_units(gtruth_setting)))\n",
    "            ax[i].set_title(label)\n",
    "            if i <= 1:\n",
    "                s.legend_.remove()\n",
    "            i += 1\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        return None, None, None\n",
    "    else:\n",
    "        label = LM_MASE\n",
    "        sc1 = hv.Scatter(sampled_data, score_settings[label][0],\n",
    "                         [label, 'class', 'datetime'])\n",
    "        sc1 = sc1.redim.range(gtruth_setting=(0, None))\n",
    "        sc1 = sc1.opts(cmap='coolwarm', size=7, title=label)\n",
    "\n",
    "        label = PM_MASE\n",
    "        sc2 = hv.Scatter(sampled_data, score_settings[label][0],\n",
    "                         [label, 'class', 'datetime'])\n",
    "        sc2 = sc2.redim.range(gtruth_setting=(0, None))\n",
    "        sc2 = sc2.opts(cmap='coolwarm', size=7, title=label)\n",
    "\n",
    "        label = PL_MASE\n",
    "        sc3 = hv.Scatter(sampled_data, score_settings[label][0],\n",
    "                         [label, 'class', 'datetime'])\n",
    "        sc3 = sc3.redim.range(gtruth_setting=(0, None))\n",
    "        sc3 = sc3.opts(cmap='coolwarm', size=7, title=label)\n",
    "\n",
    "        return sc1, sc2, sc3\n",
    "\n",
    "scaled_error = scaled_error.rename({'micro cells/mL relative abundance_x': 'micro cells/mL relative abundance',\n",
    "                               'lab gtruth relative abundance_x': 'lab gtruth relative abundance' }, axis=1)\n",
    "\n",
    "data = scaled_error.copy()\n",
    "scores_df = defaultdict(list)\n",
    "counts = rel_counts\n",
    "\n",
    "score_settings = {LM_MASE: (counts[0], counts[1]),\n",
    "                  PM_MASE: (counts[0], counts[2]),\n",
    "                  PL_MASE: (counts[1], counts[2])}\n",
    "\n",
    "\n",
    "# plot_class_mase_scores_vs_gtruth_count_for_each_setting(sampled_data, score_settings)\n",
    "\n",
    "print('\\nInteractive')\n",
    "sc1, sc2, sc3 = plot_class_mase_scores_vs_gtruth_count_for_each_setting(data,\n",
    "                                                                        score_settings,\n",
    "                                                                        interactive=False)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "i = 0\n",
    "for label, (gtruth_setting, experimental_setting) in score_settings.items():\n",
    "    ax[i].hist(data[gtruth_setting], bins=100)\n",
    "#     s = sns.scatterplot(x=gtruth_setting, y=label, hue='class',\n",
    "#                         data=sampled_data, ax=ax[i], s=50)\n",
    "    ax[i].set_yscale('symlog')\n",
    "    ax[i].set_xscale('symlog')\n",
    "#     ax[i].set_ylabel('MASE')\n",
    "#     ax[i].set_xlabel(\n",
    "#         \"Logged Gtruth Counts ({})\".format(get_units(gtruth_setting)))\n",
    "#     ax[i].set_title(label)\n",
    "#     if i <= 1:\n",
    "#         s.legend_.remove()\n",
    "    i += 1\n",
    "\n",
    "\n",
    "hv.Layout(sc1 + sc2 + sc3).cols(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from counts_analysis.c_utils import get_units\n",
    "\n",
    "def plot_class_smape_scores_vs_gtruth_count_combined_settings(data, score_settings):\n",
    "    label = LM_MASE\n",
    "    sm = data[[score_settings[label][0], 'class', 'datetime', LM_MASE, PM_MASE]]\n",
    "    sm = sm.melt(id_vars=[score_settings[label][0], 'class', 'datetime'], var_name=['setting'], value_name='mase')\n",
    "\n",
    "    current_palette_7 = sns.color_palette(\"coolwarm\", 7)\n",
    "    sns.set_palette(current_palette_7)\n",
    "    label = LM_MASE\n",
    "    markers = {LM_MASE: \"X\", PM_MASE: \"^\"}\n",
    "    sns.scatterplot(x=score_settings[label][0], y='mase', hue='class', markers=markers, style='setting', data=sm, s=75)\n",
    "    plt.xscale('symlog')\n",
    "    plt.yscale('symlog')\n",
    "    plt.xlabel('Logged Gtruth Micro ({})'.format(get_units(score_settings[label][0])))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=score_settings[label][0], y='mase', hue='class', markers=markers, style='setting', data=sm, s=75)\n",
    "    # plt.xscale('symlog')\n",
    "    plt.xlabel('Gtruth Micro ({})'.format(get_units(score_settings[label][0])))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "plot_class_smape_scores_vs_gtruth_count_combined_settings(scaled_error, score_settings)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Seasonal/NonSeasonal Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mase_errors = scaled_error.append(scaled_error1)\n",
    "mase_errors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_smape_scores_vs_gtruth_count_combined_settings(mase_errors, score_settings); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab_rnd",
   "language": "python",
   "name": "hab_rnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
