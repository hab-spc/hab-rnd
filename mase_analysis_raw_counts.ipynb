{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plt_colors\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "import scipy.stats as st\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "from holoviews import dim\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rc('xtick', labelsize=14)     \n",
    "matplotlib.rc('ytick', labelsize=14)\n",
    "matplotlib.rc('axes', labelsize=14, titlesize=14)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from counts_analysis.c_utils import COUNTS_CSV, CLASSES, set_settings, set_counts\n",
    "\n",
    "#== Load Datasets ==#\n",
    "df = pd.read_csv(COUNTS_CSV['counts'])\n",
    "# Dataset without problematic classes (Gyrodinium, Pseudo-nitzchia chain)\n",
    "df_ = df[df['class'].isin(CLASSES)].reset_index(drop=True)\n",
    "data = df.copy()\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#=== Set count forms & settings ===#\n",
    "# COUNT\n",
    "# SETTING\n",
    "# Original raw counts\n",
    "volumetric_counts = set_counts('gtruth', 'cells/mL', micro_default=True)\n",
    "rc_counts = set_counts('gtruth', 'raw count', micro_default=True)\n",
    "rc_counts_pred = set_counts('predicted', 'raw count', micro_default=True)\n",
    "\n",
    "raw_counts = set_counts('gtruth', 'raw count', micro_default=False)\n",
    "raw_counts_pred = set_counts('predicted', 'raw count', micro_default=False)\n",
    "\n",
    "rc_settings = set_settings(rc_counts)\n",
    "print('Example of setting\\n{}'.format(rc_settings))\n",
    "# Relative abundance\n",
    "rel_counts = set_counts('gtruth', 'relative abundance', micro_default=False)\n",
    "rel_counts = ['micro cells/mL relative abundance'] + list(rel_counts[1:])\n",
    "# Classifier predicted counts\n",
    "rel_counts_pred = set_counts('predicted', 'relative abundance', micro_default=False)\n",
    "rel_counts_pred = ['micro cells/mL relative abundance'] + list(rel_counts_pred[1:])\n",
    "\n",
    "#=== Set classifier gtruth vs predictions\n",
    "lab_gtruth_pred = ['lab {} raw count'.format(lbl) for lbl in ['gtruth', 'predicted']]\n",
    "pier_gtruth_pred = ['pier {} raw count'.format(lbl) for lbl in ['gtruth', 'predicted']]\n",
    "\n",
    "def compute_relative_abundance(raw_count, data):\n",
    "    if 'micro' in raw_count:\n",
    "        relative_column = 'micro cells/mL relative abundance'\n",
    "    else:\n",
    "        relative_column = f'{raw_count.split()[0]} {raw_count.split()[1]} relative abundance'\n",
    "    data[relative_column] = data.groupby('class')[raw_count].apply(lambda x: x / x.sum() * 100.0 if sum(x) != 0 else x)\n",
    "    return data\n",
    "\n",
    "def filter_classes(df, classes):\n",
    "    return df[~df['class'].isin(classes)].reset_index(drop=True)\n",
    "\n",
    "def load_absl_counts_dataset(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_baseline_dataset(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "absl_counts = load_absl_counts_dataset(df.copy())\n",
    "baseline = load_baseline_dataset(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Raw Counts Dataset\n",
    "\"\"\"\n",
    "y = baseline.copy()\n",
    "COUNTS = raw_counts\n",
    "dataset_type = 'RAW_COUNTS'.upper()\n",
    "printmd(f'# {dataset_type} ERROR ANALYSIS')\n",
    "printmd('Camera Counts')\n",
    "display(y[['class', 'datetime'] + list(COUNTS)].head(10))\n",
    "# printmd('Automated Classifier Counts')\n",
    "# display(y[['class', 'datetime'] + list(rc_counts_pred)].head(10))\n",
    "\n",
    "#=== plot distributions ===#\n",
    "from counts_analysis.plot_class_summary import plot_summary_sampling_class_dist\n",
    "# printmd('Original Relative Abundance')\n",
    "# plot_summary_sampling_class_dist(df, rel_counts, False)\n",
    "printmd(f'### {dataset_type} Camera Distribution')\n",
    "plot_summary_sampling_class_dist(y, COUNTS, True, relative=False)\n",
    "\n",
    "# printmd(f'### {dataset_type} Automated Classifier Counts Distribution')\n",
    "# plot_summary_sampling_class_dist(y, rc_counts_pred, True, relative=False)\n",
    "\n",
    "from validate_exp.stat_fns import mase, investigate_mase, pearson, concordance_correlation_coefficient\n",
    "\n",
    "# ms = investigate_mase(y.groupby('class').get_group('Prorocentrum micans'), gtruth=rc_counts[0], pred=rc_counts[1])\n",
    "# ms['scaled_error'] = ms['error'] / ms['naive']\n",
    "# display(ms)\n",
    "# print(np.mean(ms['scaled_error']))\n",
    "\n",
    "# Set evaluation metric\n",
    "stat = mase\n",
    "\n",
    "# Set settings\n",
    "settings_ = [set_settings(count) for count in [raw_counts, raw_counts_pred]]\n",
    "count_forms = dict(zip(['raw_counts', 'raw_counts_pred'], settings_))\n",
    "\n",
    "from eval_counts import compare_count_forms\n",
    "\n",
    "# Evaluate count forms\n",
    "printmd(f'# {dataset_type} MASE')\n",
    "settings_score = compare_count_forms(count_forms, stat, y)\n",
    "\n",
    "# Evaluate count forms\n",
    "printmd(f'# {dataset_type} Pearson')\n",
    "settings_score = compare_count_forms(count_forms, pearson, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS = raw_counts\n",
    "\n",
    "def _compute_mase(cls_df, x, y, error_name):\n",
    "    ms = investigate_mase(cls_df, gtruth=x, pred=y)\n",
    "    ms[error_name] = ms['error'] / ms['naive']\n",
    "    return ms\n",
    "\n",
    "LM_MASE = 'MASE (lab - micro)'\n",
    "PM_MASE = 'MASE (pier - micro)'\n",
    "PL_MASE = 'MASE (pier - lab)'\n",
    "\n",
    "cls = 'Lingulodinium polyedra'\n",
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=COUNTS[1], y=COUNTS[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])} ({np.std(lm[LM_MASE])})\\n')\n",
    "printmd('### error pier - micro')\n",
    "display(pm)\n",
    "print(f'Final Score (pier - micro): {np.mean(pm[PM_MASE])} ({np.std(pm[PM_MASE])})\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (pier - lab): {np.mean(pl[PL_MASE])} ({np.std(pl[PL_MASE])})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS = raw_counts_pred\n",
    "\n",
    "def _compute_mase(cls_df, x, y, error_name):\n",
    "    ms = investigate_mase(cls_df, gtruth=x, pred=y)\n",
    "    ms[error_name] = ms['error'] / ms['naive']\n",
    "    return ms\n",
    "\n",
    "LM_MASE = 'MASE (lab - micro)'\n",
    "PM_MASE = 'MASE (pier - micro)'\n",
    "PL_MASE = 'MASE (pier - lab)'\n",
    "\n",
    "cls = 'Lingulodinium polyedra'\n",
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=COUNTS[1], y=COUNTS[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} (CLASSIFIER) Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])} ({np.std(lm[LM_MASE])})\\n')\n",
    "printmd('### error pier - micro')\n",
    "display(pm)\n",
    "print(f'Final Score (pier - micro): {np.mean(pm[PM_MASE])} ({np.std(pm[PM_MASE])})\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (pier - lab): {np.mean(pl[PL_MASE])} ({np.std(pl[PL_MASE])})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = 'Prorocentrum micans'\n",
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=COUNTS[1], y=COUNTS[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])} ({np.std(lm[LM_MASE])})\\n')\n",
    "printmd('### error pier - micro')\n",
    "display(pm)\n",
    "print(f'Final Score (pier - micro): {np.mean(pm[PM_MASE])} ({np.std(pm[PM_MASE])})\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (pier - lab): {np.mean(pl[PL_MASE])} ({np.std(pl[PL_MASE])})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS = raw_counts_pred\n",
    "\n",
    "cls = 'Prorocentrum micans'\n",
    "cls_df = y.groupby('class').get_group(cls)\n",
    "lm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[1], error_name=LM_MASE)\n",
    "pm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[2], error_name=PM_MASE)\n",
    "pl = _compute_mase(cls_df, x=COUNTS[1], y=COUNTS[2], error_name=PL_MASE)\n",
    "\n",
    "printmd(f'# {cls} Error Analysis')\n",
    "printmd('### error lab - micro')\n",
    "display(lm)\n",
    "print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])} ({np.std(lm[LM_MASE])})\\n')\n",
    "printmd('### error pier - micro')\n",
    "display(pm)\n",
    "print(f'Final Score (pier - micro): {np.mean(pm[PM_MASE])} ({np.std(pm[PM_MASE])})\\n')\n",
    "printmd('### error pier - lab')\n",
    "display(pl)\n",
    "print(f'Final Score (pier - lab): {np.mean(pl[PL_MASE])} ({np.std(pl[PL_MASE])})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))\n",
    "    \n",
    "def _compute_mase(cls_df, x, y, error_name):\n",
    "    ms = investigate_mase(cls_df, gtruth=x, pred=y)\n",
    "    ms[error_name] = ms['error'] / ms['naive']\n",
    "    return ms\n",
    "\n",
    "COUNTS = raw_counts\n",
    "\n",
    "LM_MASE = 'MASE (lab - micro)'\n",
    "PM_MASE = 'MASE (pier - micro)'\n",
    "PL_MASE = 'MASE (pier - lab)'\n",
    "\n",
    "scaled_error = pd.DataFrame()\n",
    "for cls, cls_df in y.groupby('class'):\n",
    "#     if cls in ['Ceratium falcatiforme or fusus', 'Ceratium furca', 'Cochlodinium']:\n",
    "#         continue\n",
    "        \n",
    "    lm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[1], error_name=LM_MASE)\n",
    "    pm = _compute_mase(cls_df, x=COUNTS[0], y=COUNTS[2], error_name=PM_MASE)\n",
    "    pl = _compute_mase(cls_df, x=COUNTS[1], y=COUNTS[2], error_name=PL_MASE)\n",
    "    printmd(f'# {cls}')\n",
    "    printmd('### error lab - micro')\n",
    "    display(lm)\n",
    "    print(f'Final Score (lab - micro): {np.mean(lm[LM_MASE])}\\n')\n",
    "    printmd('### error pier - lab')\n",
    "    display(pl)\n",
    "    print(f'Final Score (lab - micro): {np.mean(pl[PL_MASE])}\\n')\n",
    "\n",
    "    cls_error = lm.merge(pm, on=['class', 'datetime'])\n",
    "    cls_error = cls_error.merge(pl, on=['class', 'datetime'])\n",
    "\n",
    "    scaled_error = scaled_error.append(cls_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models.formatters import DatetimeTickFormatter\n",
    "formatter = DatetimeTickFormatter(months='%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts HeatMap[colorbar=True, width=1000, height=300, xrotation=60, tools=['hover'], shared_axes=True, fontscale=1.5]\n",
    "\n",
    "import hvplot.pandas\n",
    "\n",
    "scaled_error1 = scaled_error.copy()\n",
    "\n",
    "scaled_error1 = scaled_error1[scaled_error1['class'].isin(['Lingulodinium polyedra', 'Prorocentrum micans'])]\n",
    "\n",
    "h1 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=LM_MASE, cmap='coolwarm').opts(title=LM_MASE)\n",
    "h2 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=PM_MASE, cmap='coolwarm').opts(title=PM_MASE)\n",
    "h3 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=PL_MASE, cmap='coolwarm').opts(title=PL_MASE)\n",
    "hv.Layout(h1 + h2 + h3).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts HeatMap[colorbar=True, width=1000, height=300, xrotation=60, tools=['hover'], shared_axes=True, fontscale=1.5]\n",
    "\n",
    "import hvplot.pandas\n",
    "\n",
    "scaled_error1 = scaled_error.copy()\n",
    "\n",
    "scaled_error1 = scaled_error1[scaled_error1['class'].isin(['Lingulodinium polyedra', 'Prorocentrum micans'])]\n",
    "\n",
    "for ms in [LM_MASE, PM_MASE, PL_MASE]:\n",
    "    condition = scaled_error1[ms] > 1.0\n",
    "    scaled_error1.loc[condition, ms] = 1.0\n",
    "\n",
    "h1 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=LM_MASE, cmap='coolwarm').opts(title=LM_MASE)\n",
    "h2 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=PM_MASE, cmap='coolwarm').opts(title=PM_MASE)\n",
    "h3 = scaled_error1.hvplot.heatmap(x='datetime', y='class', C=PL_MASE, cmap='coolwarm').opts(title=PL_MASE)\n",
    "hv.Layout(h1 + h2 + h3).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hab_rnd",
   "language": "python",
   "name": "hab_rnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
